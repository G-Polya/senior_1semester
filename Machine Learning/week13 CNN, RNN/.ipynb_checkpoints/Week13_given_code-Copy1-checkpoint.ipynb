{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCCk8_dHpuNf"
   },
   "source": [
    "# Week13 Deep Learning and TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "teh54kPruHeI"
   },
   "source": [
    "# 1. Convolutional Neural Netowks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lL0ZR2l3ItrO"
   },
   "source": [
    "### Image Classification with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mthideHn1bPR"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8Ncqy7jvenD"
   },
   "source": [
    "### Load and Preprocess the Fashion-MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fyQZJQSuKBP"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziqAmALKuRxL"
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "num_classes = 10\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) # Expand dimension (Channel dim)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heYH1Ssdu67x"
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfbdIbV2u7fm"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(y_train.shape[0], 1) \n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fILY-2KMu75I"
   },
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(y_test.shape[0], 1) \n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Rxz6wkKvHGT"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZiVGDjjgvLol"
   },
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZW5kLyWv2UC"
   },
   "outputs": [],
   "source": [
    "layers = tf.keras.layers\n",
    "base_model = None\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rB3FZusjwGJk"
   },
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hGLF0IeItrj"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VM5sHFb6wpEf"
   },
   "outputs": [],
   "source": [
    "base_history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "55pMLSXEwzzk"
   },
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5jlhgreNw3Cr"
   },
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-yYfjONzBjA"
   },
   "source": [
    "### The model with dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pn1fV8iezTiv"
   },
   "outputs": [],
   "source": [
    "# Dropout Model\n",
    "dropout_model = None\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0C22AQ_nzZG8"
   },
   "outputs": [],
   "source": [
    "drop_history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFOmpzfN31yY"
   },
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtOcXvTK33gj"
   },
   "outputs": [],
   "source": [
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAT_RVIARomJ"
   },
   "source": [
    "### Plotting the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UuzP0HSfyGw8"
   },
   "outputs": [],
   "source": [
    "def plot_history(histories, key='loss'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                       '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "                 label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "\n",
    "plot_history([('Base CNNs', base_history),\n",
    "              ('Dropout CNNs', drop_history)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# 2. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6bvTAJB1mvT"
   },
   "source": [
    "### Character-level Language Model with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Load and preprocess the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIrsNR2DKnj3"
   },
   "source": [
    "**1. Download the Shakespeare's Sonnet dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ycgAOlHJ233"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
    "'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# Load whole text file as a string, then decode.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Me3S89faJ9ho"
   },
   "outputs": [],
   "source": [
    "# We'll use the subset\n",
    "text = text[:14592]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXSyfmOoWAcM"
   },
   "outputs": [],
   "source": [
    "# Take a look first 250 characters\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpvakzOMItr5"
   },
   "outputs": [],
   "source": [
    "# The unique characters in the file\n",
    "vocab = None\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcDKb91DKl_o"
   },
   "source": [
    "**2. Vectorize the text**\n",
    "- **text_as_int** : a vector with shape of (14592, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices, and vice versa\n",
    "char2idx = None\n",
    "idx2char = None\n",
    "\n",
    "# Convert the characters to the indices\n",
    "text_as_int = None\n",
    "\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvaB92hOLn00"
   },
   "source": [
    "**3. Creating training task**\n",
    "- **char_dataset** : `tf.data.Dataset` object, shape = (14592, )\n",
    "   - .take(*count*) : Creates a `Dataset` with at most *count* elements from this dataset.\n",
    "   - .batch(*count*) : Combines consecutive elements of this dataset into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = None\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euDVCoMhX-dS"
   },
   "outputs": [],
   "source": [
    "# Check the shape : char_dataset\n",
    "np.array(list(char_dataset.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRmrxW7AYUpS"
   },
   "source": [
    "- **sequences** : `tf.data.Dataset` object, shape = (144, 101)\n",
    "   - `.map(map_func)` : Maps `map_func` across the elements of this dataset.\n",
    "- **dataset** : `tf.data.Dataset` object, shape = (144, 2, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [],
   "source": [
    "# 'batch' method convert these individual characters to sequences of the desired size\n",
    "sequences = None\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFG2uz2lY2UO"
   },
   "outputs": [],
   "source": [
    "# Check the shape : sequences\n",
    "np.array(list(sequences.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "# map_func\n",
    "def split_input_target(chunk):\n",
    "    # input text is shifted to form the target text \n",
    "    input_text = None\n",
    "    target_text = None\n",
    "    return input_text, target_text\n",
    "\n",
    "# 'map' method lets us easily apply a simple function to each batch\n",
    "dataset = None\n",
    "\n",
    "# Print the examples \n",
    "for input_ex, target_ex in  dataset.take(1):\n",
    "    print ('Input : ', repr(''.join(idx2char[input_ex.numpy()])))\n",
    "    print ('Target :', repr(''.join(idx2char[target_ex.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUMVeSf6Z6rC"
   },
   "outputs": [],
   "source": [
    "# Check the shape : dataset - (1)\n",
    "np.array(list(dataset.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hw97GdSnbI7s"
   },
   "source": [
    "4. Create training batches\n",
    "- **dataset** : `tf.data.Dataset` object, shape = (9, 2, 16, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 100\n",
    "\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3s10Ndpae7Y"
   },
   "outputs": [],
   "source": [
    "# Check the shape : dataset - (2)\n",
    "np.array(list(dataset.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "- 3 layers are used to define this model\n",
    "    1. `tf.keras.layers.Embedding`: The input layer, A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
    "    2. `tf.keras.layers.RNN`: A RNN with size `units=rnn_units`\n",
    "    3. `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    layers = tf.keras.layers\n",
    "    model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baOROwVlSFYD"
   },
   "outputs": [],
   "source": [
    "# Build the model \n",
    "model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Check the model architecture\n",
    "# Model can be run on inputs of any length \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "### Try the model\n",
    "Before training the model, take a look about how does the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [],
   "source": [
    "# Check the shape of the input, output\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = None\n",
    "\n",
    "print(input_example_batch.shape)\n",
    "print(target_example_batch.shape)\n",
    "print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "# We need to sample from the output distribution, not to take the argmax of the distribution\n",
    "sampled_indices = None\n",
    "sampled_indices = None\n",
    "\n",
    "# Predictions of the next character index \n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWcFwPwLSo05"
   },
   "outputs": [],
   "source": [
    "# Decode the predictions, the model shows poor performance \n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "# 1. Attach an optimizer, and a loss function\n",
    "# define the loss function \n",
    "def loss(labels, logits):\n",
    "    # Because the output of model is logit, \n",
    "    return None\n",
    "\n",
    "# Configure the training procedure\n",
    "optimizer = None\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# 2. Configure the checkpoints\n",
    "# `tf.keras.callbacks.ModelCheckpoint` : The callback function to save the model checkpoint\n",
    "\n",
    "# Directory where the model weights will be saved\n",
    "ckpt_dir = './training_rnns_ckpts'\n",
    "\n",
    "# Checkpoint name\n",
    "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt_rnns_{epoch}\")\n",
    "\n",
    "# Callback function to save the model weights\n",
    "ckpt_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [],
   "source": [
    "# 3. Execute the training\n",
    "EPOCHS=10\n",
    "rnn_history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [],
   "source": [
    "# Check the latest checkpoint\n",
    "tf.train.latest_checkpoint(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "# To run the model with one sample(not with batch_size of samples),\n",
    "# We rebuild the model, and load the weights from the saved checkpoint. \n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(ckpt_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLnghlJemP_h"
   },
   "outputs": [],
   "source": [
    "# Check the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "# The prediction loop\n",
    "def generate_text(model, start_string):\n",
    "    # Number of characters to generate\n",
    "    n_generate = 1000\n",
    "\n",
    "    # Converting start_strings to index (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Making the empty list to store results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(n_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Passing the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn"
   },
   "outputs": [],
   "source": [
    "# Generate text from start string\n",
    "print(generate_text(model, start_string=\"All: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "06kb_yXJYFRJ"
   },
   "source": [
    "# Text Classifiation with RNNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kiD1HZQwmw-o"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Co5nbEHctsd"
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TMvwMvE8nGCk"
   },
   "source": [
    "### Load and preprocess the IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUXVl9chc5OM"
   },
   "outputs": [],
   "source": [
    "# using tfds library, load the imdb dataset\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                          as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WDhdFAVtdGkY"
   },
   "outputs": [],
   "source": [
    "# info object has the lookup table(encoder) of token and index\n",
    "encoder = info.features['text'].encoder\n",
    "print('Vocabulary size: {}'.format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtlZw3nYd0h6"
   },
   "outputs": [],
   "source": [
    "# Test the encoder\n",
    "sample_string = 'Hello TensorFlow.'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: \"{}\"'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njJEwjQHd-W2"
   },
   "outputs": [],
   "source": [
    "# Test the encoder\n",
    "for index in encoded_string:\n",
    "  print('{} ----> {}'.format(index, encoder.decode([index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V0MGZNNieA61"
   },
   "outputs": [],
   "source": [
    "# Creating training task\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDEHvQ3ueB8e"
   },
   "outputs": [],
   "source": [
    "# Check the shape of batches\n",
    "for x, y in train_dataset.take(2):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(\"-\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-D3TEHd-pmn8"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eM3Zdh79eHKr"
   },
   "outputs": [],
   "source": [
    "layers = tf.keras.layers\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(encoder.vocab_size, 64),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXlLSgeu0Z7Y"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsVlLGlCpo_o"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X1TxzddMeJ8R"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=5,\n",
    "                    validation_data=test_dataset, \n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tkx_4t1aq1BH"
   },
   "source": [
    "### Evalutate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1akGAPUhIEu"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K1XKq2DKnBlq"
   },
   "outputs": [],
   "source": [
    "# helper function \n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMDsHaHrmhqO"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sW28SoXAmi33"
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5iyTe_ks2AQ"
   },
   "outputs": [],
   "source": [
    "def sample_pred(text, model):\n",
    "    encoded_text = encoder.encode(text)\n",
    "    encoded_text = tf.cast(encoded_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_text, 0))\n",
    "    prob = tf.sigmoid(predictions)[0][0].numpy()\n",
    "    print('Prob : ', prob)\n",
    "    if prob >= 0.5:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uiI8nDsryxm"
   },
   "outputs": [],
   "source": [
    "sample_pred_text = 'You should watch this movie, this movie is excellent'\n",
    "sample_pred(sample_pred_text, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lsg8Nx1hU6ku"
   },
   "source": [
    "# Quiz 1 : Image Classification Model on the CIFAR-10\n",
    "- Build the Convolutional Neural Networks\n",
    "    - Build the model following the bellow model summary\n",
    "    - Apply the dropout regularization to the model and compare the result\n",
    "- Compare the performance of the model built last week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rmtvrC6Ug2P"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2ryFJ7GRZew"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "olux865vUkeq"
   },
   "source": [
    "### Load the CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vcGNnJw5RgJ6"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vZUrjDtRRhbq"
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3i1E2yuUphN"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fi3F21v8Rlwf"
   },
   "outputs": [],
   "source": [
    "layers = tf.keras.layers\n",
    "\n",
    "cifar_model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSQ8Kin7TSdF"
   },
   "outputs": [],
   "source": [
    "cifar_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ln_sV_9lUyGZ"
   },
   "outputs": [],
   "source": [
    "# Compile the model(set optimizer, loss function and metrics)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1aZ68EkU5EJ"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu_dXxCvTexl"
   },
   "outputs": [],
   "source": [
    "cifar_history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCy9Bm4cU6sG"
   },
   "source": [
    "### Apply the dropout to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LiUGqcp3VIsF"
   },
   "outputs": [],
   "source": [
    "cifar_drop_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQctkOHRVcIJ"
   },
   "outputs": [],
   "source": [
    "cifar_drop_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQT4dVNsVnmL"
   },
   "outputs": [],
   "source": [
    "# Compile the model(set optimizer, loss function and metrics)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JZ-QEi8VgE4"
   },
   "outputs": [],
   "source": [
    "cifar_drop_history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PAxtOy5V504"
   },
   "outputs": [],
   "source": [
    "def plot_history(histories, key='loss'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                       '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "                 label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "\n",
    "plot_history([('Base CNNs', cifar_history),\n",
    "              ('Dropout CNNs', cifar_drop_history)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovC7aP0nXwst"
   },
   "outputs": [],
   "source": [
    "def plot_history(histories, key='accuracy'):\n",
    "    plt.figure(figsize=(16,10))\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                       '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "                 label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "\n",
    "plot_history([('Base CNNs', cifar_history),\n",
    "              ('Dropout CNNs', cifar_drop_history)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjXv3VaVkz-4"
   },
   "source": [
    "# Quiz 2 : Character-level Language Model\n",
    "- Build the Character-level Language Model with LSTM\n",
    "- Compare the generated text to the one generated by RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvC8C066DQai"
   },
   "source": [
    "### Character-level Language Model with RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ohUtdhvCDQaj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRt71_SHDQak"
   },
   "source": [
    "### Load and preprocess the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "074LL6IHDQal"
   },
   "source": [
    "**1. Download the Shakespeare's Sonnet dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJweCMV9DQal"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
    "'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# Load whole text file as a string, then decode.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8k1p44oKDQao"
   },
   "outputs": [],
   "source": [
    "# We'll use the subset\n",
    "text = text[:14592]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MngruZvhDQap"
   },
   "outputs": [],
   "source": [
    "# Take a look first 250 characters\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbE93KrWDQar"
   },
   "outputs": [],
   "source": [
    "# The unique characters in the file\n",
    "vocab = None\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIo2yBgfDQat"
   },
   "source": [
    "**2. Vectorize the text**\n",
    "- **text_as_int** : a vector with shape of (14592, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVcXPXsBDQat"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices, and vice versa\n",
    "char2idx = None\n",
    "idx2char = None\n",
    "\n",
    "# Convert the characters to the indices\n",
    "text_as_int = None\n",
    "\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MxA4DypADQav"
   },
   "source": [
    "**3. Creating training task**\n",
    "- **char_dataset** : `tf.data.Dataset` object, shape = (14592, )\n",
    "   - .take(*count*) : Creates a `Dataset` with at most *count* elements from this dataset.\n",
    "   - .batch(*count*) : Combines consecutive elements of this dataset into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvtSWukdDQav"
   },
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = None\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBMm2xSBDQax"
   },
   "outputs": [],
   "source": [
    "# Check the shape : char_dataset\n",
    "np.array(list(char_dataset.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eb-g_lTXDQay"
   },
   "source": [
    "- **sequences** : `tf.data.Dataset` object, shape = (144, 101)\n",
    "   - `.map(map_func)` : Maps `map_func` across the elements of this dataset.\n",
    "- **dataset** : `tf.data.Dataset` object, shape = (144, 2, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKPBGhrXDQay"
   },
   "outputs": [],
   "source": [
    "# 'batch' method convert these individual characters to sequences of the desired size\n",
    "sequences = None\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUVt_KlBDQa1"
   },
   "outputs": [],
   "source": [
    "# Check the shape : sequences\n",
    "np.array(list(sequences.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWVmT2ElDQa3"
   },
   "outputs": [],
   "source": [
    "# map_func\n",
    "def split_input_target(chunk):\n",
    "    # input text is shifted to form the target text \n",
    "    input_text = None\n",
    "    target_text = None\n",
    "    return input_text, target_text\n",
    "\n",
    "# 'map' method lets us easily apply a simple function to each batch\n",
    "dataset = None\n",
    "\n",
    "# Print the examples \n",
    "for input_ex, target_ex in  dataset.take(1):\n",
    "    print ('Input : ', repr(''.join(idx2char[input_ex.numpy()])))\n",
    "    print ('Target :', repr(''.join(idx2char[target_ex.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHlcFIkXDQa4"
   },
   "outputs": [],
   "source": [
    "# Check the shape : dataset - (1)\n",
    "np.array(list(dataset.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "II1MGyYiDQa6"
   },
   "source": [
    "4. Create training batches\n",
    "- **dataset** : `tf.data.Dataset` object, shape = (9, 2, 16, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ji7Qf1KJDQa6"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 100\n",
    "\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiQuC_jgDQa8"
   },
   "outputs": [],
   "source": [
    "# Check the shape : dataset - (2)\n",
    "np.array(list(dataset.as_numpy_iterator())).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2pd7ENcDQa-"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXYG990xDQa_"
   },
   "source": [
    "- 3 layers are used to define this model\n",
    "    1. `tf.keras.layers.Embedding`: The input layer, A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
    "    2. `tf.keras.layers.RNN`: A RNN with size `units=rnn_units`\n",
    "    3. `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziYtAFxzDQa_"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ceC_1J2DQbC"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    layers = tf.keras.layers\n",
    "    model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prlSUUJzDQbD"
   },
   "outputs": [],
   "source": [
    "# Build the model \n",
    "model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n",
    "\n",
    "# Check the model architecture\n",
    "# Model can be run on inputs of any length \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9EIJSrdDQbF"
   },
   "source": [
    "### Try the model\n",
    "Before training the model, take a look about how does the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GZWvojkDQbF"
   },
   "outputs": [],
   "source": [
    "# Check the shape of the input, output\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = None\n",
    "\n",
    "print(input_example_batch.shape)\n",
    "print(target_example_batch.shape)\n",
    "print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euX8a6fiDQbH"
   },
   "outputs": [],
   "source": [
    "# We need to sample from the output distribution, not to take the argmax of the distribution\n",
    "sampled_indices = None\n",
    "sampled_indices = None\n",
    "\n",
    "# Predictions of the next character index \n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nC4V-RXnDQbK"
   },
   "outputs": [],
   "source": [
    "# Decode the predictions, the model shows poor performance \n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YrhM_tOEDQbL"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB2_SjNWDQbL"
   },
   "outputs": [],
   "source": [
    "# 1. Attach an optimizer, and a loss function\n",
    "# define the loss function \n",
    "def loss(labels, logits):\n",
    "    # Because the output of model is logit, \n",
    "    return None\n",
    "\n",
    "# Configure the training procedure\n",
    "optimizer = None\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxpLTcawDQbN"
   },
   "outputs": [],
   "source": [
    "# 2. Configure the checkpoints\n",
    "# `tf.keras.callbacks.ModelCheckpoint` : The callback function to save the model checkpoint\n",
    "\n",
    "# Directory where the model weights will be saved\n",
    "ckpt_dir = './training_lstm_ckpts'\n",
    "\n",
    "# Checkpoint name\n",
    "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt_lstm_{epoch}\")\n",
    "\n",
    "# Callback function to save the model weights\n",
    "ckpt_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJdDstuQDQbP"
   },
   "outputs": [],
   "source": [
    "# 3. Execute the training\n",
    "EPOCHS=10\n",
    "rnn_history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBFfGZJ9DQbS"
   },
   "source": [
    "### Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0pCdtoKDQbS"
   },
   "outputs": [],
   "source": [
    "# Check the latest checkpoint\n",
    "tf.train.latest_checkpoint(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_KPRFjnIDQbU"
   },
   "outputs": [],
   "source": [
    "# To run the model with one sample(not with batch_size of samples),\n",
    "# We rebuild the model, and load the weights from the saved checkpoint. \n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(ckpt_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DanRhk2aDQbW"
   },
   "outputs": [],
   "source": [
    "# Check the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37hMxWvYDQbX"
   },
   "outputs": [],
   "source": [
    "# The prediction loop\n",
    "def generate_text(model, start_string):\n",
    "    # Number of characters to generate\n",
    "    n_generate = 1000\n",
    "\n",
    "    # Converting start_strings to index (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Making the empty list to store results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(n_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Passing the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KjaJyki2DQbZ"
   },
   "outputs": [],
   "source": [
    "# Generate text from start string\n",
    "print(generate_text(model, start_string=\"All: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1r1TtxnJDCdy"
   },
   "outputs": [],
   "source": [
    "# The prediction loop\n",
    "def generate_text(model, start_string):\n",
    "    # Number of characters to generate\n",
    "    n_generate = 1000\n",
    "\n",
    "    # Converting start_strings to index (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Making the empty list to store results\n",
    "    text_generated = []\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(n_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Passing the predicted character as the next input to the model along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_Iwfrn0DCd2"
   },
   "outputs": [],
   "source": [
    "# Generate text from start string\n",
    "print(generate_text(model, start_string=\"All: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BkbbnY0ZeDX"
   },
   "outputs": [],
   "source": [
    "# 2. Configure the checkpoints\n",
    "# `tf.keras.callbacks.ModelCheckpoint` : The callback function to save the model checkpoint\n",
    "\n",
    "# Directory where the model weights will be saved\n",
    "ckpt_dir = './training_lstm_ckpts'\n",
    "\n",
    "# Checkpoint name\n",
    "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt_lstm_{epoch}\")\n",
    "\n",
    "# Callback function to save the model weights\n",
    "ckpt_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_prefix,\n",
    "    save_weights_only=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week13_given_code",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
