{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\"alt.atheism\", \"soc.religion.christian\", \"comp.graphics\", \"sci.med\"]\n",
    "\n",
    "news = fetch_20newsgroups(categories=categories, shuffle=False, random_state=42)\n",
    "\n",
    "X= news.data\n",
    "y = news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"From: keith@cco.caltech.edu (Keith Allan Schneider)\\nSubject: Re: <Political Atheists?\\nOrganization: California Institute of Technology, Pasadena\\nLines: 14\\nNNTP-Posting-Host: lloyd.caltech.edu\\n\\nbobbe@vice.ICO.TEK.COM (Robert Beauchaine) writes:\\n\\n>To show that the examples I and others\\n>have provided are *not* counter examples of your supposed inherent\\n>moral hypothesis, you have to successfully argue that\\n>domestication removes or alters this morality.\\n\\nI think that domestication will change behavior to a large degree.\\nDomesticated animals exhibit behaviors not found in the wild.  I\\ndon't think that they can be viewed as good representatives of the\\nwild animal kingdom, since they have been bred for thousands of years\\nto produce certain behaviors, etc.\\n\\nkeith\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wlska\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wlska\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"<[^>]*>\",\"\", text)\n",
    "    text = re.sub(\"[\\W]+\",\" \",text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    return [porter.stem(word) for word in text_tokens]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=preprocessor, tokenizer=tokenizer_porter, stop_words=stop, max_df=0.1, min_df=10)\n",
    "X_train_vector = tfidf.fit_transform(X_train)\n",
    "X_test_vector = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1579, 2796)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X_train_vector = X_train_vector.toarray()\n",
    "X_test_vector = X_test_vector.toarray()\n",
    "X_train_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(verbose=1)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty=\"l2\", verbose=1)\n",
    "lr.fit(X_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9911336288790373"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "lr.score(X_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9542772861356932"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "lr.score(X_test_vector, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 0, 3], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "docs = ['The outbreak was declared a global pandemic by the World Health Organization (WHO) on 11 March.',\n",
    "\n",
    "'Today, computer graphics is a core technology in digitalphotography, film, video games, cell phone and computer displays,and many specialized applications.',\n",
    "\n",
    "'Arguments for atheism range from philosophical to social and historical approaches.',\n",
    "\n",
    "'The Bible is a compilation of many shorter books written at different times by a variety of authors, and later assembled into the biblical canon.'\n",
    "]\n",
    "\n",
    "test = tfidf.transform(docs)\n",
    "lr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "tree.score(X_train_vector, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8362831858407079"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "tree.score(X_test_vector, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3, 1, 0, 2], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "tree.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}