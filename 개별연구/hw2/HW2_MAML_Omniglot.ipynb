{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import google_drive_downloader...!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "    print(\"import google_drive_downloader...!\")\n",
    "except ModuleNotFoundError:\n",
    "    !pip install googledrivedownloader\n",
    "    from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "    print(\"import downloaded google_drive_downloader...!\")\n",
    "    \n",
    "# Need to download the Omniglot dataset -- DON'T MODIFY THIS CELL\n",
    "if not os.path.isdir('./omniglot_resized'):\n",
    "    gdd.download_file_from_google_drive(file_id='1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI',\n",
    "                                        dest_path='./omniglot_resized.zip',\n",
    "                                        unzip=True)\n",
    "    \n",
    "assert os.path.isdir('./omniglot_resized')\n",
    "\n",
    "# select GPU number!!\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Utility functions. \"\"\"\n",
    "## NOTE: You do not need to modify this block but you will need to use it.\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "## Loss utilities\n",
    "def cross_entropy_loss(pred, label, k_shot):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=tf.stop_gradient(label)) / k_shot)\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(labels, predictions), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layers used by MAML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Convolutional layers used by MAML model.\"\"\"\n",
    "## NOTE: You do not need to modify this block but you will need to use it.\n",
    "seed = 123\n",
    "def conv_block(inp, cweight, bweight, bn, activation=tf.nn.relu, residual=False):\n",
    "    \"\"\" Perform, conv, batch norm, nonlinearity, and max pool \"\"\"\n",
    "    stride, no_stride = [1,2,2,1], [1,1,1,1]\n",
    "  \n",
    "    conv_output = tf.nn.conv2d(input=inp, filters=cweight, strides=no_stride, padding='SAME') + bweight\n",
    "    normed = bn(conv_output)\n",
    "    normed = activation(normed)\n",
    "    return normed\n",
    "\n",
    "class ConvLayers(tf.keras.layers.Layer):\n",
    "    def __init__(self, channels, dim_hidden, dim_output, img_size):\n",
    "        super(ConvLayers, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_output = dim_output\n",
    "        self.img_size = img_size\n",
    "    \n",
    "        weights = {}\n",
    "    \n",
    "        dtype = tf.float32\n",
    "        weight_initializer =  tf.keras.initializers.GlorotUniform()\n",
    "        k = 3\n",
    "    \n",
    "        weights['conv1'] = tf.Variable(weight_initializer(shape=[k, k, self.channels, self.dim_hidden]), name='conv1', dtype=dtype)\n",
    "        weights['b1'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b1')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(name='bn1')\n",
    "        weights['conv2'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv2', dtype=dtype)\n",
    "        weights['b2'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b2')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(name='bn2')\n",
    "        weights['conv3'] = tf.Variable(weight_initializer(shape=[k, k, self.dim_hidden, self.dim_hidden]), name='conv3', dtype=dtype)\n",
    "        weights['b3'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b3')\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(name='bn3')\n",
    "        weights['conv4'] = tf.Variable(weight_initializer([k, k, self.dim_hidden, self.dim_hidden]), name='conv4', dtype=dtype)\n",
    "        weights['b4'] = tf.Variable(tf.zeros([self.dim_hidden]), name='b4')\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization(name='bn4')\n",
    "        weights['w5'] = tf.Variable(weight_initializer(shape=[self.dim_hidden, self.dim_output]), name='w5', dtype=dtype)\n",
    "        weights['b5'] = tf.Variable(tf.zeros([self.dim_output]), name='b5')\n",
    "        self.conv_weights = weights\n",
    "\n",
    "    def call(self, inp, weights):\n",
    "        channels = self.channels\n",
    "        inp = tf.reshape(inp, [-1, self.img_size, self.img_size, channels])\n",
    "        hidden1 = conv_block(inp, weights['conv1'], weights['b1'], self.bn1)\n",
    "        hidden2 = conv_block(hidden1, weights['conv2'], weights['b2'], self.bn2)\n",
    "        hidden3 = conv_block(hidden2, weights['conv3'], weights['b3'], self.bn3)\n",
    "        hidden4 = conv_block(hidden3, weights['conv4'], weights['b4'], self.bn4)\n",
    "        hidden4 = tf.reduce_mean(input_tensor=hidden4, axis=[1, 2])\n",
    "        return tf.matmul(hidden4, weights['w5']) + weights['b5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Data loading scripts\"\"\"\n",
    "## NOTE: You do not need to modify this block but you will need to use it.\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from scipy import misc\n",
    "import imageio\n",
    "\n",
    "def get_images(paths, labels, n_samples=None, shuffle=True):\n",
    "    \"\"\"\n",
    "    Takes a set of character folders and labels and returns paths to image files\n",
    "    paired with labels.\n",
    "    Args:\n",
    "        paths: A list of character folders\n",
    "        labels: List or numpy array of same length as paths\n",
    "        n_samples: Number of images to retrieve per character\n",
    "    Returns:\n",
    "        List of (label, image_path) tuples\n",
    "    \"\"\"\n",
    "    if n_samples is not None:\n",
    "        sampler = lambda x: random.sample(x, n_samples)\n",
    "    else:\n",
    "        sampler = lambda x: x\n",
    "    images_labels = [(i, os.path.join(path, image))\n",
    "                     for i, path in zip(labels, paths)\n",
    "                     for image in sampler(os.listdir(path))]\n",
    "    if shuffle:\n",
    "        random.shuffle(images_labels)\n",
    "    return images_labels\n",
    "\n",
    "\n",
    "def image_file_to_array(filename, dim_input):\n",
    "    \"\"\"\n",
    "    Takes an image path and returns numpy array\n",
    "    Args:\n",
    "        filename: Image filename\n",
    "        dim_input: Flattened shape of image\n",
    "    Returns:\n",
    "        1 channel image\n",
    "    \"\"\"\n",
    "    image = imageio.imread(filename)\n",
    "    image = image.reshape([dim_input])\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = 1.0 - image\n",
    "    return image\n",
    "\n",
    "\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, num_meta_test_classes, num_meta_test_samples_per_class, config={}):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of classes for classification (K-way)\n",
    "            num_samples_per_class: num samples to generate per class in one batch\n",
    "            num_meta_test_classes: Number of classes for classification (K-way) at meta-test time\n",
    "            num_meta_test_samples_per_class: num samples to generate per class in one batch at meta-test time\n",
    "            batch_size: size of meta batch size (e.g. number of functions)\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "        self.num_meta_test_samples_per_class = num_meta_test_samples_per_class\n",
    "        self.num_meta_test_classes = num_meta_test_classes\n",
    "    \n",
    "        data_folder = config.get('data_folder', './omniglot_resized')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "    \n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "    \n",
    "        character_folders = [os.path.join(data_folder, family, character)\n",
    "                             for family in os.listdir(data_folder)\n",
    "                             if os.path.isdir(os.path.join(data_folder, family))\n",
    "                             for character in os.listdir(os.path.join(data_folder, family))\n",
    "                             if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "    \n",
    "        random.seed(123)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[: num_train]\n",
    "        self.metaval_character_folders = character_folders[num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[num_train + num_val:]\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size, shuffle=True, swap=False):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "            batch_type: meta_train/meta_val/meta_test\n",
    "            shuffle: randomly shuffle classes or not\n",
    "            swap: swap number of classes (N) and number of samples per class (K) or not\n",
    "        Returns:\n",
    "            A a tuple of (1) Image batch and (2) Label batch where\n",
    "            image batch has shape [B, N, K, 784] and label batch has shape [B, N, K, N] if swap is False\n",
    "            where B is batch size, K is number of samples per class, N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"meta_train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        elif batch_type == \"meta_val\":\n",
    "            folders = self.metaval_character_folders\n",
    "            num_classes = self.num_classes\n",
    "            num_samples_per_class = self.num_samples_per_class\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "            num_classes = self.num_meta_test_classes\n",
    "            num_samples_per_class = self.num_meta_test_samples_per_class\n",
    "        all_image_batches, all_label_batches = [], []\n",
    "        for i in range(batch_size):\n",
    "            sampled_character_folders = random.sample(folders, num_classes)\n",
    "            labels_and_images = get_images(sampled_character_folders, range(num_classes), n_samples=num_samples_per_class, shuffle=False)\n",
    "            labels = [li[0] for li in labels_and_images]\n",
    "            images = [image_file_to_array(li[1], self.dim_input) for li in labels_and_images]\n",
    "            images = np.stack(images)\n",
    "            labels = np.array(labels).astype(np.int32)\n",
    "            labels = np.reshape(labels, (num_classes, num_samples_per_class))\n",
    "            labels = np.eye(num_classes, dtype=np.float32)[labels]\n",
    "            images = np.reshape(images, (num_classes, num_samples_per_class, -1))\n",
    "            \n",
    "            batch = np.concatenate([labels, images], 2)\n",
    "            \n",
    "            if shuffle:\n",
    "                for p in range(num_samples_per_class):\n",
    "                    np.random.shuffle(batch[:, p])\n",
    "            labels = batch[:, :, :num_classes]\n",
    "            images = batch[:, :, num_classes:]\n",
    "            \n",
    "            if swap:\n",
    "                labels = np.swapaxes(labels, 0, 1)\n",
    "                images = np.swapaxes(images, 0, 1)\n",
    "            all_image_batches.append(images)\n",
    "            all_label_batches.append(labels)\n",
    "            \n",
    "        all_image_batches = np.stack(all_image_batches)\n",
    "        all_label_batches = np.stack(all_label_batches)\n",
    "        return all_image_batches, all_label_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Check (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "####################\n",
      "meta-batch 0: \n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_way = 5\n",
    "k_shot = 2\n",
    "data_path = './omniglot_resized'\n",
    "meta_batch_size = 25\n",
    "\n",
    "data_generator = DataGenerator(n_way, k_shot*2, n_way, k_shot*2, config={'data_folder': data_path})\n",
    "\n",
    "all_image_batches, all_label_batches = data_generator.sample_batch(batch_type='meta_train', batch_size=meta_batch_size, shuffle=False, swap=False)\n",
    "\n",
    "input_tr, input_ts = all_image_batches[:,:,:k_shot,:], all_image_batches[:,:,k_shot:,:]\n",
    "label_tr, label_ts = all_label_batches[:,:,:k_shot,:], all_label_batches[:,:,k_shot:,:]\n",
    "\n",
    "def visualize_sample_data(images, label):\n",
    "    for b in range(len(images)): # batch size\n",
    "        print(\"#\"*20)\n",
    "        print(\"meta-batch {}: \".format(b))\n",
    "        print(\"#\"*20)\n",
    "        label\n",
    "        count = 0\n",
    "        fig = plt.figure(figsize=(15, 7))\n",
    "        for sa in range(k_shot): # k-shot\n",
    "            for cl in range(n_way): # n-way\n",
    "                plt.subplot(k_shot, n_way, count + 1)\n",
    "                plt.title(\"Class {} - Example {} \\n {}\".format(cl, sa, label[b, cl, sa]))\n",
    "                image = images[b, cl, sa].reshape((28,28))\n",
    "                plt.imshow(image)\n",
    "                plt.axis('off');\n",
    "                count += 1\n",
    "        break # first meta batch sample visualization\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "print(\"training data\")        \n",
    "images = input_tr\n",
    "label = label_tr\n",
    "visualize_sample_data(images, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data\n",
      "####################\n",
      "meta-batch 0: \n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "print(\"testing data\")        \n",
    "images = input_ts\n",
    "label = label_ts\n",
    "visualize_sample_data(images, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"MAML model code\"\"\"\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class MAML(tf.keras.Model):\n",
    "    def __init__(self, dim_input=1, dim_output=1, \n",
    "                 num_inner_updates=1,\n",
    "                 inner_update_lr=0.4, num_filters=32, \n",
    "                 k_shot=5, learn_inner_update_lr=False):\n",
    "        super(MAML, self).__init__()\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "        self.inner_update_lr = inner_update_lr\n",
    "        self.loss_func = partial(cross_entropy_loss, k_shot=k_shot)\n",
    "        self.dim_hidden = num_filters\n",
    "        self.channels = 1\n",
    "        self.img_size = int(np.sqrt(self.dim_input/self.channels))\n",
    "    \n",
    "        # outputs_ts[i] and losses_ts_post[i] are the output and loss after i+1 inner gradient updates\n",
    "        losses_tr_pre, outputs_tr, losses_ts_post, outputs_ts = [], [], [], []\n",
    "        accuracies_tr_pre, accuracies_ts = [], []\n",
    "    \n",
    "        # for each loop in the inner training loop\n",
    "        outputs_ts = [[]]*num_inner_updates\n",
    "        losses_ts_post = [[]]*num_inner_updates\n",
    "        accuracies_ts = [[]]*num_inner_updates\n",
    "    \n",
    "        # Define the weights - these should NOT be directly modified by the\n",
    "        # inner training loop\n",
    "        tf.random.set_seed(seed)\n",
    "        self.conv_layers = ConvLayers(self.channels, self.dim_hidden, self.dim_output, self.img_size)\n",
    "    \n",
    "        self.learn_inner_update_lr = learn_inner_update_lr\n",
    "        if self.learn_inner_update_lr:\n",
    "            self.inner_update_lr_dict = {}\n",
    "            for key in self.conv_layers.conv_weights.keys():\n",
    "                self.inner_update_lr_dict[key] = [tf.Variable(self.inner_update_lr, name='inner_update_lr_%s_%d' % (key, j)) for j in range(num_inner_updates)]\n",
    "  \n",
    "    #@tf.function\n",
    "    def call(self, inp, meta_batch_size=25, num_inner_updates=1):\n",
    "        def task_inner_loop(inp, reuse=True,\n",
    "                      meta_batch_size=25, num_inner_updates=1):\n",
    "            \"\"\"\n",
    "            Perform gradient descent for one task in the meta-batch (i.e. inner-loop).\n",
    "                Args:\n",
    "                    inp: a tuple (input_tr, input_ts, label_tr, label_ts), where input_tr and label_tr are the inputs and\n",
    "                        labels used for calculating inner loop gradients and input_ts and label_ts are the inputs and\n",
    "                        labels used for evaluating the model after inner updates.\n",
    "                        Should be shapes:\n",
    "                            input_tr: [N*K, 784]\n",
    "                            input_ts: [N*K, 784]\n",
    "                            label_tr: [N*K, N]\n",
    "                            label_ts: [N*K, N]\n",
    "                Returns:\n",
    "                    task_output: a list of outputs, losses and accuracies at each inner update\n",
    "            \"\"\"\n",
    "            # the inner and outer loop data\n",
    "            input_tr, input_ts, label_tr, label_ts = inp\n",
    "            \n",
    "            # weights corresponds to the initial weights in MAML (i.e. the meta-parameters)\n",
    "            weights = self.conv_layers.conv_weights\n",
    "            fast_weights = weights.copy()\n",
    "            \n",
    "            # the predicted outputs, loss values, and accuracy for the pre-update model (with the initial weights)\n",
    "            # evaluated on the inner loop training data\n",
    "            task_output_tr_pre, task_loss_tr_pre, task_accuracy_tr_pre = None, None, None\n",
    "      \n",
    "            # lists to keep track of outputs, losses, and accuracies of test data for each inner_update\n",
    "            # where task_outputs_ts[i], task_losses_ts[i], task_accuracies_ts[i] are the output, loss, and accuracy\n",
    "            # after i+1 inner gradient updates\n",
    "            task_outputs_ts, task_losses_ts, task_accuracies_ts = [], [], []\n",
    "        \n",
    "            #######################################################################################\n",
    "            # perform num_inner_updates to get modified weights\n",
    "            # modified weights should be used to evaluate performance\n",
    "            # Note that at each inner update, always use input_tr and label_tr for calculating gradients\n",
    "            # and use input_ts and labels for evaluating performance\n",
    "      \n",
    "            # HINTS: You will need to use tf.GradientTape().\n",
    "            # Read through the tf.GradientTape() documentation to see how 'persistent' should be set.\n",
    "            # Here is some documentation that may be useful: \n",
    "            # https://www.tensorflow.org/guide/advanced_autodiff#higher-order_gradients\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
    "            for i in range(num_inner_updates):\n",
    "                with tf.GradientTape(persistent=True) as inner_tape:\n",
    "                    # calcuating meta_train loss with training data [input_tr, label_tr]\n",
    "                    # inner_tape.watch(fast_weights)\n",
    "                    task_output_tr_pre = self.conv_layers(inp=input_tr, weights=fast_weights)\n",
    "                    task_loss_tr_pre = self.loss_func(task_output_tr_pre, label_tr)\n",
    "\n",
    "                    \n",
    "                    # end of with block\n",
    "                    \n",
    "                #############################\n",
    "                ####      Question       #### \n",
    "                #### YOUR CODE GOES HERE ####\n",
    "\n",
    "                # 1) calculating gradients for inner task with testing data\n",
    "                gradients = inner_tape.gradient(task_loss_tr_pre, fast_weights)\n",
    "                # 2) learning fast wegiths with inner task gradients (meta parameter \"self.conv_layers.conv_weights\" shouldn't be updated!)\n",
    "                for key in fast_weights.keys():\n",
    "                    if self.learn_inner_update_lr:\n",
    "                        fast_weights[key] = fast_weights[key] - self.inner_update_lr_dict[key][i] * gradients[key]\n",
    "                    else:\n",
    "                        fast_weights[key] = fast_weights[key] - self.inner_update_lr * gradients[key]\n",
    "                # calcuating adaptation loss(task_outputs_ts, task_losses_ts) with testing data [input_ts, label_ts]\n",
    "                task_outputs_ts.append(self.conv_layers(input_ts, fast_weights)) \n",
    "                task_losses_ts.append(self.loss_func(task_outputs_ts[-1], label_ts))\n",
    "                \n",
    "                #############################\n",
    "            \n",
    "            #######################################################################################\n",
    "      \n",
    "            # Compute accuracies from output predictions\n",
    "            task_accuracy_tr_pre = accuracy(tf.argmax(input=label_tr, axis=1), tf.argmax(input=tf.nn.softmax(task_output_tr_pre), axis=1))\n",
    "            \n",
    "            for j in range(num_inner_updates):\n",
    "                task_accuracies_ts.append(accuracy(tf.argmax(input=label_ts, axis=1), tf.argmax(input=tf.nn.softmax(task_outputs_ts[j]), axis=1)))\n",
    "            \n",
    "            # Task Output\n",
    "            ## task_output_tr_pre    -> tf.float32\n",
    "            ## task_outputs_ts       -> [tf.float32]*num_inner_updates\n",
    "            ## task_loss_tr_pre      -> tf.float32\n",
    "            ## task_losses_ts        -> [tf.float32]*num_inner_updates\n",
    "            ## task_accuracy_tr_pre  -> tf.float32\n",
    "            ## task_accuracies_ts    -> [tf.float32]*num_inner_updates\n",
    "            \n",
    "            task_output = [task_output_tr_pre, task_outputs_ts, task_loss_tr_pre, task_losses_ts, task_accuracy_tr_pre, task_accuracies_ts]\n",
    "             \n",
    "            return task_output\n",
    "\n",
    "        input_tr, input_ts, label_tr, label_ts = inp\n",
    "        \n",
    "        # to initialize the batch norm vars, might want to combine this, and not run idx 0 twice.\n",
    "        unused = task_inner_loop((input_tr[0], input_ts[0], label_tr[0], label_ts[0]),\n",
    "                                 False,\n",
    "                                 meta_batch_size,\n",
    "                                 num_inner_updates)\n",
    "        out_dtype = [tf.float32, [tf.float32]*num_inner_updates, tf.float32, [tf.float32]*num_inner_updates]\n",
    "        out_dtype.extend([tf.float32, [tf.float32]*num_inner_updates])\n",
    "        \n",
    "        task_inner_loop_partial = partial(task_inner_loop, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
    "        \n",
    "        result = tf.map_fn(task_inner_loop_partial,\n",
    "                           elems=(input_tr, input_ts, label_tr, label_ts),\n",
    "                           dtype=out_dtype,\n",
    "                           parallel_iterations=meta_batch_size)    \n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Model training code\"\"\"\n",
    "\"\"\"\n",
    "Usage Instructions:\n",
    "  5-way, 1-shot omniglot:\n",
    "    python main.py --meta_train_iterations=15000 --meta_batch_size=25 --k_shot=1 --inner_update_lr=0.4 --num_inner_updates=1 --logdir=logs/omniglot5way/\n",
    "  20-way, 1-shot omniglot:\n",
    "    python main.py --meta_train_iterations=15000 --meta_batch_size=16 --k_shot=1 --n_way=20 --inner_update_lr=0.1 --num_inner_updates=5 --logdir=logs/omniglot20way/\n",
    "  To run evaluation, use the '--meta_train=False' flag and the '--meta_test_set=True' flag to use the meta-test set.\n",
    "\"\"\"\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "def outer_train_step(inp, model, optim, meta_batch_size=25, num_inner_updates=1):\n",
    "    with tf.GradientTape(persistent=False) as outer_tape:\n",
    "        result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
    "    \n",
    "        outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
    "    \n",
    "        total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
    "\n",
    "    gradients = outer_tape.gradient(total_losses_ts[-1], model.trainable_variables)\n",
    "    optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  \n",
    "    total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
    "    total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
    "    total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
    "\n",
    "    return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts\n",
    "\n",
    "def outer_eval_step(inp, model, meta_batch_size=25, num_inner_updates=1):\n",
    "    result = model(inp, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
    "  \n",
    "    outputs_tr, outputs_ts, losses_tr_pre, losses_ts, accuracies_tr_pre, accuracies_ts = result\n",
    "  \n",
    "    total_loss_tr_pre = tf.reduce_mean(losses_tr_pre)\n",
    "    total_losses_ts = [tf.reduce_mean(loss_ts) for loss_ts in losses_ts]\n",
    "  \n",
    "    total_accuracy_tr_pre = tf.reduce_mean(accuracies_tr_pre)\n",
    "    total_accuracies_ts = [tf.reduce_mean(accuracy_ts) for accuracy_ts in accuracies_ts]\n",
    "  \n",
    "    return outputs_tr, outputs_ts, total_loss_tr_pre, total_losses_ts, total_accuracy_tr_pre, total_accuracies_ts  \n",
    "\n",
    "\n",
    "def meta_train_fn(model, exp_string, data_generator,\n",
    "               n_way=5, meta_train_iterations=15000, meta_batch_size=25,\n",
    "               log=True, logdir='./log/model', k_shot=1, num_inner_updates=1, meta_lr=0.001):\n",
    "    SUMMARY_INTERVAL = 10\n",
    "    SAVE_INTERVAL = 100\n",
    "    PRINT_INTERVAL = 10  \n",
    "    TEST_PRINT_INTERVAL = PRINT_INTERVAL*5\n",
    "  \n",
    "    pre_accuracies, post_accuracies = [], []\n",
    "  \n",
    "    num_classes = data_generator.num_classes\n",
    "    img_size = data_generator.img_size\n",
    "  \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=meta_lr)\n",
    "\n",
    "    for itr in range(meta_train_iterations):\n",
    "    \n",
    "        # sample a batch of training data and partition into\n",
    "        # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
    "        # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
    "        \n",
    "        all_image_batches, all_label_batches = data_generator.sample_batch(batch_type='meta_train', batch_size=meta_batch_size, shuffle=False, swap=False)\n",
    "        \n",
    "        input_tr, input_ts = all_image_batches[:,:,:k_shot,:], all_image_batches[:,:,k_shot:,:]\n",
    "        label_tr, label_ts = all_label_batches[:,:,:k_shot,:], all_label_batches[:,:,k_shot:,:]\n",
    "        \n",
    "        # reshape input tensor\n",
    "        input_tr = tf.reshape(input_tr, [-1, n_way*k_shot, img_size[0]*img_size[0]])\n",
    "        input_ts = tf.reshape(input_ts, [-1, n_way*k_shot, img_size[0]*img_size[0]])\n",
    "        label_tr = tf.reshape(label_tr, [-1, n_way*k_shot, n_way])\n",
    "        label_ts = tf.reshape(label_ts, [-1, n_way*k_shot, n_way])\n",
    "    \n",
    "    \n",
    "        inp = (input_tr, input_ts, label_tr, label_ts)\n",
    "        \n",
    "        result = outer_train_step(inp, model, optimizer, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
    "    \n",
    "        if itr % SUMMARY_INTERVAL == 0:\n",
    "            pre_accuracies.append(result[-2])\n",
    "            post_accuracies.append(result[-1][-1])\n",
    "    \n",
    "        if (itr!=0) and itr % PRINT_INTERVAL == 0:\n",
    "            print_str = 'Iteration %d: pre-inner-loop train accuracy: %.5f, post-inner-loop test accuracy: %.5f' % (itr, np.mean(pre_accuracies), np.mean(post_accuracies))\n",
    "            print(print_str)\n",
    "            pre_accuracies, post_accuracies = [], []\n",
    "    \n",
    "        if (itr!=0) and itr % TEST_PRINT_INTERVAL == 0:\n",
    "      \n",
    "            # sample a batch of validation data and partition it into\n",
    "            # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
    "            # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
    "        \n",
    "            all_image_batches, all_label_batches = data_generator.sample_batch(batch_type='meta_val', batch_size=meta_batch_size, shuffle=False, swap=False)\n",
    "\n",
    "            input_tr, input_ts = all_image_batches[:,:,:k_shot,:], all_image_batches[:,:,k_shot:,:]\n",
    "            label_tr, label_ts = all_label_batches[:,:,:k_shot,:], all_label_batches[:,:,k_shot:,:]\n",
    "            \n",
    "            # reshape input tensor\n",
    "            input_tr = tf.reshape(input_tr, [-1, n_way*k_shot, img_size[0]*img_size[0]])\n",
    "            input_ts = tf.reshape(input_ts, [-1, n_way*k_shot, img_size[0]*img_size[0]])\n",
    "            label_tr = tf.reshape(label_tr, [-1, n_way*k_shot, n_way])\n",
    "            label_ts = tf.reshape(label_ts, [-1, n_way*k_shot, n_way])\n",
    "    \n",
    "      \n",
    "            inp = (input_tr, input_ts, label_tr, label_ts)\n",
    "            result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
    "      \n",
    "            print('Meta-validation pre-inner-loop train accuracy: %.5f, meta-validation post-inner-loop test accuracy: %.5f' % (result[-2], result[-1][-1]))\n",
    "\n",
    "        \n",
    "            model_file = logdir + '/' + exp_string +  '/model' + str(itr)\n",
    "            print(\"Saving to \", model_file)\n",
    "            model.save_weights(model_file)\n",
    "    \n",
    "    model_file = logdir + '/' + exp_string +  '/model' + str(itr)\n",
    "    print(\"Saving to \", model_file)\n",
    "    model.save_weights(model_file)\n",
    "\n",
    "# calculated for omniglot\n",
    "NUM_META_TEST_POINTS = 600\n",
    "\n",
    "def meta_test_fn(model, data_generator, n_way=5, \n",
    "                 meta_batch_size=25, k_shot=1,\n",
    "                 num_inner_updates=1):\n",
    "  \n",
    "    num_classes = data_generator.num_classes\n",
    "    img_size = data_generator.img_size\n",
    "  \n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "  \n",
    "    meta_test_accuracies = []\n",
    "  \n",
    "    for _ in range(NUM_META_TEST_POINTS):\n",
    "    \n",
    "        # sample a batch of test data and partition it into\n",
    "        # the support/training set (input_tr, label_tr) and the query/test set (input_ts, label_ts)\n",
    "        # NOTE: The code assumes that the support and query sets have the same number of examples.\n",
    "        \n",
    "        all_image_batches, all_label_batches = data_generator.sample_batch(batch_type='meta_test', batch_size=meta_batch_size, shuffle=False, swap=False)\n",
    "        \n",
    "        input_tr, input_ts = all_image_batches[:,:,:k_shot,:], all_image_batches[:,:,k_shot:,:]\n",
    "        label_tr, label_ts = all_label_batches[:,:,:k_shot,:], all_label_batches[:,:,k_shot:,:]\n",
    "        \n",
    "        # reshape input tensor\n",
    "        input_tr = tf.reshape(input_tr, [-1, n_way*k_shot, img_size[0]*img_size[0]])\n",
    "        input_ts = tf.reshape(input_ts, [-1, n_way*k_shot, img_size[0]*img_size[0]])\n",
    "        label_tr = tf.reshape(label_tr, [-1, n_way*k_shot, n_way])\n",
    "        label_ts = tf.reshape(label_ts, [-1, n_way*k_shot, n_way])\n",
    "\n",
    "        \n",
    "        inp = (input_tr, input_ts, label_tr, label_ts)\n",
    "        result = outer_eval_step(inp, model, meta_batch_size=meta_batch_size, num_inner_updates=num_inner_updates)\n",
    "    \n",
    "        meta_test_accuracies.append(result[-1][-1])\n",
    "\n",
    "    meta_test_accuracies = np.array(meta_test_accuracies)\n",
    "    means = np.mean(meta_test_accuracies)\n",
    "    stds = np.std(meta_test_accuracies)\n",
    "    ci95 = 1.96*stds/np.sqrt(NUM_META_TEST_POINTS)\n",
    "  \n",
    "    print('Mean meta-test accuracy/loss, stddev, and confidence intervals')\n",
    "    print((means, stds, ci95))\n",
    "\n",
    "\n",
    "def run_maml(n_way=5, k_shot=1, meta_batch_size=25, meta_lr=0.001,\n",
    "             inner_update_lr=0.4, num_filters=32, num_inner_updates=1,\n",
    "             learn_inner_update_lr=False,\n",
    "             resume=False, resume_itr=0, log=True, logdir='./log/model',\n",
    "             data_path='./omniglot_resized',meta_train=True,\n",
    "             meta_train_iterations=15000, meta_train_k_shot=-1,\n",
    "             meta_train_inner_update_lr=-1):\n",
    "\n",
    "\n",
    "    # call data_generator and get data with k_shot*2 samples per class\n",
    "    data_generator = DataGenerator(n_way, k_shot*2, n_way, k_shot*2, config={'data_folder': data_path})\n",
    "  \n",
    "    # set up MAML model\n",
    "    dim_output = data_generator.dim_output\n",
    "    dim_input = data_generator.dim_input\n",
    "    model = MAML(dim_input,\n",
    "                dim_output,\n",
    "                num_inner_updates=num_inner_updates,\n",
    "                inner_update_lr=inner_update_lr,\n",
    "                k_shot=k_shot,\n",
    "                num_filters=num_filters,\n",
    "                learn_inner_update_lr=learn_inner_update_lr)\n",
    "\n",
    "    if meta_train_k_shot == -1:\n",
    "        meta_train_k_shot = k_shot\n",
    "    if meta_train_inner_update_lr == -1:\n",
    "        meta_train_inner_update_lr = inner_update_lr\n",
    "\n",
    "    exp_string = 'cls_'+str(n_way)+'.mbs_'+str(meta_batch_size) + '.k_shot_' + str(meta_train_k_shot) + '.inner_numstep_' + str(num_inner_updates) + '.inner_updatelr_' + str(meta_train_inner_update_lr) + '.learn_inner_update_lr_' + str(learn_inner_update_lr)\n",
    "\n",
    "    if meta_train:\n",
    "        meta_train_fn(model, exp_string, data_generator,\n",
    "                      n_way, meta_train_iterations, meta_batch_size, log, logdir,\n",
    "                      k_shot, num_inner_updates, meta_lr)\n",
    "    else:\n",
    "        meta_batch_size = 1\n",
    "    \n",
    "        model_file = tf.train.latest_checkpoint(logdir + '/' + exp_string)\n",
    "        print(\"Restoring model weights from \", model_file)\n",
    "        model.load_weights(model_file)\n",
    "    \n",
    "        meta_test_fn(model, data_generator, n_way, meta_batch_size, k_shot, num_inner_updates)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Setting parallel_iterations > 1 has no effect when executing eagerly. Consider calling map_fn with tf.function to execute fn in parallel.\n",
      "Iteration 10: pre-inner-loop train accuracy: 0.19600, post-inner-loop test accuracy: 0.20800\n",
      "Iteration 20: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.20000\n",
      "Iteration 30: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.18400\n",
      "Iteration 40: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.17600\n",
      "Iteration 50: pre-inner-loop train accuracy: 0.13600, post-inner-loop test accuracy: 0.21600\n",
      "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.20000\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model50\n",
      "Iteration 60: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.20000\n",
      "Iteration 70: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.20000\n",
      "Iteration 80: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.23200\n",
      "Iteration 90: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.22400\n",
      "Iteration 100: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.20000\n",
      "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.21600\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model100\n",
      "Iteration 110: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.19200\n",
      "Iteration 120: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.22400\n",
      "Iteration 130: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.23200\n",
      "Iteration 140: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.28000\n",
      "Iteration 150: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.24800\n",
      "Meta-validation pre-inner-loop train accuracy: 0.16000, meta-validation post-inner-loop test accuracy: 0.24000\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model150\n",
      "Iteration 160: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.30400\n",
      "Iteration 170: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.30400\n",
      "Iteration 180: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.27200\n",
      "Iteration 190: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.24000\n",
      "Iteration 200: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.23200\n",
      "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.24000\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model200\n",
      "Iteration 210: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.31200\n",
      "Iteration 220: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.28000\n",
      "Iteration 230: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.28000\n",
      "Iteration 240: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.28800\n",
      "Iteration 250: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.30400\n",
      "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.24800\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model250\n",
      "Iteration 260: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.33600\n",
      "Iteration 270: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.36000\n",
      "Iteration 280: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.31200\n",
      "Iteration 290: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.30400\n",
      "Iteration 300: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.32000\n",
      "Meta-validation pre-inner-loop train accuracy: 0.16800, meta-validation post-inner-loop test accuracy: 0.26400\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model300\n",
      "Iteration 310: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.31200\n",
      "Iteration 320: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.32800\n",
      "Iteration 330: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.36000\n",
      "Iteration 340: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.30400\n",
      "Iteration 350: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.33600\n",
      "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.28000\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model350\n",
      "Iteration 360: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.27200\n",
      "Iteration 370: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.36800\n",
      "Iteration 380: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.28000\n",
      "Iteration 390: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40000\n",
      "Iteration 400: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.33600\n",
      "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.31200\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model400\n",
      "Iteration 410: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.29600\n",
      "Iteration 420: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.36000\n",
      "Iteration 430: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.34400\n",
      "Iteration 440: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.34400\n",
      "Iteration 450: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.32800\n",
      "Meta-validation pre-inner-loop train accuracy: 0.20800, meta-validation post-inner-loop test accuracy: 0.34400\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model450\n",
      "Iteration 460: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 470: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.32000\n",
      "Iteration 480: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.36800\n",
      "Iteration 490: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.35200\n",
      "Iteration 500: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.35200\n",
      "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.35200\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model500\n",
      "Iteration 510: pre-inner-loop train accuracy: 0.12000, post-inner-loop test accuracy: 0.34400\n",
      "Iteration 520: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.28000\n",
      "Iteration 530: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.35200\n",
      "Iteration 540: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.32000\n",
      "Iteration 550: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.32000\n",
      "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.32800\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model550\n",
      "Iteration 560: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.32800\n",
      "Iteration 570: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.30400\n",
      "Iteration 580: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.32800\n",
      "Iteration 590: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.33600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 600: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.34400\n",
      "Meta-validation pre-inner-loop train accuracy: 0.22400, meta-validation post-inner-loop test accuracy: 0.31200\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model600\n",
      "Iteration 610: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.33600\n",
      "Iteration 620: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.37600\n",
      "Iteration 630: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.38400\n",
      "Iteration 640: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.36000\n",
      "Iteration 650: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.34400\n",
      "Meta-validation pre-inner-loop train accuracy: 0.12800, meta-validation post-inner-loop test accuracy: 0.36800\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model650\n",
      "Iteration 660: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.32800\n",
      "Iteration 670: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.36000\n",
      "Iteration 680: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.42400\n",
      "Iteration 690: pre-inner-loop train accuracy: 0.15200, post-inner-loop test accuracy: 0.36000\n",
      "Iteration 700: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.32800\n",
      "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.35200\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model700\n",
      "Iteration 710: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.35200\n",
      "Iteration 720: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.36000\n",
      "Iteration 730: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.34400\n",
      "Iteration 740: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.38400\n",
      "Iteration 750: pre-inner-loop train accuracy: 0.27200, post-inner-loop test accuracy: 0.39200\n",
      "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.39200\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model750\n",
      "Iteration 760: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.40000\n",
      "Iteration 770: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.43200\n",
      "Iteration 780: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.37600\n",
      "Iteration 790: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.36800\n",
      "Iteration 800: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.38400\n",
      "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.39200\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model800\n",
      "Iteration 810: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 820: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.38400\n",
      "Iteration 830: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 840: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.35200\n",
      "Iteration 850: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.36000\n",
      "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.38400\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model850\n",
      "Iteration 860: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.43200\n",
      "Iteration 870: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 880: pre-inner-loop train accuracy: 0.26400, post-inner-loop test accuracy: 0.41600\n",
      "Iteration 890: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.35200\n",
      "Iteration 900: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.41600\n",
      "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.45600\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model900\n",
      "Iteration 910: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.40800\n",
      "Iteration 920: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.37600\n",
      "Iteration 930: pre-inner-loop train accuracy: 0.20000, post-inner-loop test accuracy: 0.42400\n",
      "Iteration 940: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.44800\n",
      "Iteration 950: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.32800\n",
      "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.46400\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model950\n",
      "Iteration 960: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.40800\n",
      "Iteration 970: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 980: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.38400\n",
      "Iteration 990: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.44800\n",
      "Iteration 1000: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.43200\n",
      "Meta-validation pre-inner-loop train accuracy: 0.19200, meta-validation post-inner-loop test accuracy: 0.39200\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model1000\n",
      "Iteration 1010: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.44800\n",
      "Iteration 1020: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.42400\n",
      "Iteration 1030: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.41600\n",
      "Iteration 1040: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.38400\n",
      "Iteration 1050: pre-inner-loop train accuracy: 0.19200, post-inner-loop test accuracy: 0.44000\n",
      "Meta-validation pre-inner-loop train accuracy: 0.18400, meta-validation post-inner-loop test accuracy: 0.37600\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model1050\n",
      "Iteration 1060: pre-inner-loop train accuracy: 0.25600, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 1070: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.45600\n",
      "Iteration 1080: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.44000\n",
      "Iteration 1090: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.46400\n",
      "Iteration 1100: pre-inner-loop train accuracy: 0.21600, post-inner-loop test accuracy: 0.46400\n",
      "Meta-validation pre-inner-loop train accuracy: 0.15200, meta-validation post-inner-loop test accuracy: 0.50400\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model1100\n",
      "Iteration 1110: pre-inner-loop train accuracy: 0.20800, post-inner-loop test accuracy: 0.40800\n",
      "Iteration 1120: pre-inner-loop train accuracy: 0.14400, post-inner-loop test accuracy: 0.44800\n",
      "Iteration 1130: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 1140: pre-inner-loop train accuracy: 0.22400, post-inner-loop test accuracy: 0.44000\n",
      "Iteration 1150: pre-inner-loop train accuracy: 0.23200, post-inner-loop test accuracy: 0.42400\n",
      "Meta-validation pre-inner-loop train accuracy: 0.20000, meta-validation post-inner-loop test accuracy: 0.49600\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model1150\n",
      "Iteration 1160: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.42400\n",
      "Iteration 1170: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.42400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1180: pre-inner-loop train accuracy: 0.16800, post-inner-loop test accuracy: 0.41600\n",
      "Iteration 1190: pre-inner-loop train accuracy: 0.18400, post-inner-loop test accuracy: 0.45600\n",
      "Iteration 1200: pre-inner-loop train accuracy: 0.17600, post-inner-loop test accuracy: 0.37600\n",
      "Meta-validation pre-inner-loop train accuracy: 0.21600, meta-validation post-inner-loop test accuracy: 0.44800\n",
      "Saving to  ./log/model/cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model1200\n",
      "Iteration 1210: pre-inner-loop train accuracy: 0.12800, post-inner-loop test accuracy: 0.39200\n",
      "Iteration 1220: pre-inner-loop train accuracy: 0.24800, post-inner-loop test accuracy: 0.47200\n",
      "Iteration 1230: pre-inner-loop train accuracy: 0.16000, post-inner-loop test accuracy: 0.44800\n",
      "Iteration 1240: pre-inner-loop train accuracy: 0.24000, post-inner-loop test accuracy: 0.41600\n"
     ]
    }
   ],
   "source": [
    "run_maml(n_way=5, k_shot=1, inner_update_lr=0.4, num_inner_updates=1, meta_train_iterations=5000, logdir='./log/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from  ./log/model//cls_5.mbs_25.k_shot_1.inner_numstep_1.inner_updatelr_0.4.learn_inner_update_lr_False/model4999\n",
      "Mean meta-test accuracy/loss, stddev, and confidence intervals\n",
      "(0.57533336, 0.17830187, 0.014267121054192181)\n"
     ]
    }
   ],
   "source": [
    "run_maml(n_way=5, k_shot=1, inner_update_lr=0.4, num_inner_updates=1, meta_train=False, logdir='./log/model/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}